{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aekanun2020/2025-AdvancedRAG/blob/main/extended_SENT_Hands_on_2_putSparseVector_into_Opensearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuQwfnOIjjSv",
        "outputId": "ad0ea5db-5ddb-4a91-8014-f9b62b8f3530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.5/264.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.5/353.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hใช้อุปกรณ์: cpu\n",
            "กำลังดาวน์โหลดไฟล์...\n",
            "กำลังดาวน์โหลด https://storage.googleapis.com/llm-course/md/1.md ไปยัง ./corpus_input/1.md\n",
            "กำลังดาวน์โหลด https://storage.googleapis.com/llm-course/md/2.md ไปยัง ./corpus_input/2.md\n",
            "กำลังดาวน์โหลด https://storage.googleapis.com/llm-course/md/44.md ไปยัง ./corpus_input/44.md\n",
            "กำลังดาวน์โหลด https://storage.googleapis.com/llm-course/md/5555.md ไปยัง ./corpus_input/5555.md\n",
            "กำลังตั้งค่า hybrid search pipeline...\n",
            "สร้าง hybrid search pipeline สำเร็จ: {'acknowledged': True}\n",
            "โหลดเอกสาร 4 ไฟล์สำเร็จ\n",
            "สร้าง 53 nodes ด้วย MarkdownNodeParser สำเร็จ\n"
          ]
        }
      ],
      "source": [
        "## !!! ก่อนรันโค้ดนี้ ผู้เรียน จำเป็นต้องเปลี่ยน OPENSEARCH_INDEX ให้เป็นชื่อของตนเอง\n",
        "## ใน format: yourname_doc_index (ตัวพิมพ์เล็ก)!!!\n",
        "\n",
        "# ติดตั้ง LlamaIndex และ dependencies\n",
        "!pip install llama-index -q\n",
        "!pip install llama-index-embeddings-huggingface -q\n",
        "!pip install llama-index-vector-stores-opensearch -q\n",
        "!pip install requests -q\n",
        "!pip install nest_asyncio -q\n",
        "\n",
        "# Import modules\n",
        "import os\n",
        "import torch\n",
        "import urllib.request\n",
        "import pickle\n",
        "import requests\n",
        "import nest_asyncio\n",
        "import json\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.opensearch import OpensearchVectorStore, OpensearchVectorClient\n",
        "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
        "\n",
        "# Apply nest_asyncio to avoid runtime errors\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# กำหนดค่าสำหรับ OpenSearch\n",
        "OPENSEARCH_ENDPOINT = \"http://34.41.37.53:9200\"\n",
        "OPENSEARCH_INDEX = \"aekanun_doc_index\"\n",
        "TEXT_FIELD = \"content\"\n",
        "EMBEDDING_FIELD = \"embedding\"\n",
        "\n",
        "# Check if CUDA is available for GPU acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ใช้อุปกรณ์: {device}\")\n",
        "\n",
        "# ฟังก์ชันสำหรับสร้าง hybrid search pipeline\n",
        "def create_hybrid_search_pipeline():\n",
        "    pipeline_url = f\"{OPENSEARCH_ENDPOINT}/_search/pipeline/hybrid-search-pipeline\"\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    pipeline_config = {\n",
        "        \"description\": \"Pipeline for hybrid search\",\n",
        "        \"phase_results_processors\": [\n",
        "            {\n",
        "                \"normalization-processor\": {\n",
        "                    \"normalization\": {\n",
        "                        \"technique\": \"min_max\"\n",
        "                    },\n",
        "                    \"combination\": {\n",
        "                        \"technique\": \"harmonic_mean\",\n",
        "                        \"parameters\": {\n",
        "                            \"weights\": [0.3, 0.7]\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.put(pipeline_url, headers=headers, data=json.dumps(pipeline_config))\n",
        "        if response.status_code in [200, 201]:\n",
        "            print(f\"สร้าง hybrid search pipeline สำเร็จ: {response.json()}\")\n",
        "        else:\n",
        "            print(f\"ไม่สามารถสร้าง pipeline ได้: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"เกิดข้อผิดพลาดในการสร้าง pipeline: {e}\")\n",
        "\n",
        "# ฟังก์ชันดาวน์โหลด corpus\n",
        "def download_corpus():\n",
        "    os.makedirs('./corpus_input', exist_ok=True)\n",
        "    urls = [\n",
        "        (\"https://storage.googleapis.com/llm-course/md/1.md\", \"./corpus_input/1.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/2.md\", \"./corpus_input/2.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/44.md\", \"./corpus_input/44.md\"),\n",
        "        (\"https://storage.googleapis.com/llm-course/md/5555.md\", \"./corpus_input/5555.md\")\n",
        "    ]\n",
        "    for url, path in urls:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"กำลังดาวน์โหลด {url} ไปยัง {path}\")\n",
        "            try:\n",
        "                urllib.request.urlretrieve(url, path)\n",
        "            except Exception as e:\n",
        "                print(f\"ไม่สามารถดาวน์โหลด {url} ได้: {e}\")\n",
        "\n",
        "# ดาวน์โหลด corpus\n",
        "print(\"กำลังดาวน์โหลดไฟล์...\")\n",
        "download_corpus()\n",
        "\n",
        "# สร้าง hybrid search pipeline\n",
        "print(\"กำลังตั้งค่า hybrid search pipeline...\")\n",
        "create_hybrid_search_pipeline()\n",
        "\n",
        "# โหลดเอกสาร Markdown จากไดเรกทอรี\n",
        "reader = SimpleDirectoryReader(\n",
        "    input_dir=\"./corpus_input\",\n",
        "    recursive=True,\n",
        "    required_exts=[\".md\", \".markdown\"]\n",
        ")\n",
        "documents = reader.load_data()\n",
        "print(f\"โหลดเอกสาร {len(documents)} ไฟล์สำเร็จ\")\n",
        "\n",
        "# สร้าง parser สำหรับ Markdown\n",
        "md_parser = MarkdownNodeParser()\n",
        "nodes = md_parser.get_nodes_from_documents(documents)\n",
        "print(f\"สร้าง {len(nodes)} nodes ด้วย MarkdownNodeParser สำเร็จ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ดูตัวอย่าง node แรก\n",
        "print(\"\\nตัวอย่าง node แรก:\")\n",
        "example_node = nodes[0]\n",
        "print(f\"Node ID: {example_node.id_}\")\n",
        "print(f\"Content (บางส่วน): {example_node.text[:100]}...\")\n",
        "print(\"Metadata:\")\n",
        "for key, value in example_node.metadata.items():\n",
        "    print(f\"  - {key}: {value}\")\n",
        "\n",
        "# ตรวจสอบว่ามี attributes อะไรบ้างใน node\n",
        "print(\"\\nAttributes ทั้งหมดของ node:\")\n",
        "for attr in dir(example_node):\n",
        "    if not attr.startswith('__'):\n",
        "        print(f\"  - {attr}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pdvVvg08baE",
        "outputId": "50df5045-496b-41a2-c4ad-d407f2cb770e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ตัวอย่าง node แรก:\n",
            "Node ID: fdab874c-b9fe-4c79-b7c5-615055ee5814\n",
            "Content (บางส่วน): # หัดเยอรมัน อาการ สาเหตุ และการรักษาโรคหัดเยอรมัน 10 วิธี !!\n",
            "\n",
            "โดย เมดไทย, เมื่อ 27 กรกฎาคม 2020 (เว...\n",
            "Metadata:\n",
            "  - file_path: /content/corpus_input/1.md\n",
            "  - file_name: 1.md\n",
            "  - file_type: text/markdown\n",
            "  - file_size: 65320\n",
            "  - creation_date: 2025-03-20\n",
            "  - last_modified_date: 2025-03-20\n",
            "  - header_path: /\n",
            "\n",
            "Attributes ทั้งหมดของ node:\n",
            "  - _abc_impl\n",
            "  - _calculate_keys\n",
            "  - _check_frozen\n",
            "  - _copy_and_set_values\n",
            "  - _get_value\n",
            "  - _iter\n",
            "  - as_related_node_info\n",
            "  - child_nodes\n",
            "  - class_name\n",
            "  - construct\n",
            "  - copy\n",
            "  - custom_model_dump\n",
            "  - dict\n",
            "  - embedding\n",
            "  - end_char_idx\n",
            "  - excluded_embed_metadata_keys\n",
            "  - excluded_llm_metadata_keys\n",
            "  - extra_info\n",
            "  - from_dict\n",
            "  - from_json\n",
            "  - from_orm\n",
            "  - get_content\n",
            "  - get_embedding\n",
            "  - get_metadata_str\n",
            "  - get_node_info\n",
            "  - get_text\n",
            "  - get_type\n",
            "  - hash\n",
            "  - id_\n",
            "  - json\n",
            "  - metadata\n",
            "  - metadata_separator\n",
            "  - metadata_seperator\n",
            "  - metadata_template\n",
            "  - mimetype\n",
            "  - model_computed_fields\n",
            "  - model_config\n",
            "  - model_construct\n",
            "  - model_copy\n",
            "  - model_dump\n",
            "  - model_dump_json\n",
            "  - model_extra\n",
            "  - model_fields\n",
            "  - model_fields_set\n",
            "  - model_json_schema\n",
            "  - model_parametrized_name\n",
            "  - model_post_init\n",
            "  - model_rebuild\n",
            "  - model_validate\n",
            "  - model_validate_json\n",
            "  - model_validate_strings\n",
            "  - next_node\n",
            "  - node_id\n",
            "  - node_info\n",
            "  - parent_node\n",
            "  - parse_file\n",
            "  - parse_obj\n",
            "  - parse_raw\n",
            "  - prev_node\n",
            "  - ref_doc_id\n",
            "  - relationships\n",
            "  - schema\n",
            "  - schema_json\n",
            "  - set_content\n",
            "  - source_node\n",
            "  - start_char_idx\n",
            "  - text\n",
            "  - text_template\n",
            "  - to_dict\n",
            "  - to_json\n",
            "  - update_forward_refs\n",
            "  - validate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ตั้งค่า embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\", device=device)\n",
        "print(f\"ตั้งค่าโมเดล embedding BAAI/bge-m3 สำเร็จ\")\n",
        "\n",
        "# ตรวจสอบขนาดของ embedding\n",
        "embeddings = embed_model.get_text_embedding(\"test\")\n",
        "dim = len(embeddings)\n",
        "print(f\"ขนาด embedding: {dim}\")\n",
        "\n",
        "# ตั้งค่า OpensearchVectorClient\n",
        "client = OpensearchVectorClient(\n",
        "    endpoint=OPENSEARCH_ENDPOINT,\n",
        "    index=OPENSEARCH_INDEX,\n",
        "    dim=dim,\n",
        "    embedding_field=EMBEDDING_FIELD,\n",
        "    text_field=TEXT_FIELD,\n",
        "    search_pipeline=\"hybrid-search-pipeline\",\n",
        ")\n",
        "print(f\"ตั้งค่า OpensearchVectorClient สำเร็จ สำหรับ index '{OPENSEARCH_INDEX}'\")\n",
        "\n",
        "# สร้าง vector store\n",
        "vector_store = OpensearchVectorStore(client)\n",
        "\n",
        "# สร้าง storage context\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# สร้าง index\n",
        "index = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "print(f\"สร้าง index สำเร็จ\")\n",
        "\n",
        "# บันทึก index ด้วย pickle\n",
        "index_filename = f\"{OPENSEARCH_INDEX}.pkl\"\n",
        "with open(index_filename, 'wb') as f:\n",
        "    pickle.dump(index, f)\n",
        "print(f\"บันทึก index ลงในไฟล์ {index_filename} สำเร็จ\")\n",
        "\n",
        "print(\"เสร็จสิ้นกระบวนการทั้งหมด!\")"
      ],
      "metadata": {
        "id": "S2-yvVDOj8xD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}